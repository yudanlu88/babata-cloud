import streamlit as st
from openai import OpenAI
from docx import Document
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import io
import asyncio
import edge_tts
from datetime import datetime

# --- 1. åŸºç¡€é…ç½® ---
st.set_page_config(page_title="å·´å·´å¡”ä¸‡èƒ½åŠ©æ‰‹", page_icon="âš¡", layout="wide")

# æ£€æŸ¥ Secrets å¯†ç 
if "DEEPSEEK_KEY" in st.secrets:
    api_key = st.secrets["DEEPSEEK_KEY"]
else:
    st.error("âš ï¸ è¯·å…ˆåœ¨ Streamlit åå°é…ç½® Secretsï¼")
    st.stop()

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

if "history" not in st.session_state: st.session_state.history = []

# --- 2. è§†è§‰ç³»ç»Ÿ ---
st.markdown("""
<style>
    .stApp { background-color: #F5F7FA; }
    .stButton>button {
        background-color: #0052D9; color: white; border-radius: 6px;
        height: 48px; font-weight: 600; width: 100%;
    }
    .stButton>button:hover { background-color: #003CAB; }
</style>
""", unsafe_allow_html=True)

# --- 3. å¼‚æ­¥è¯­éŸ³å‡½æ•° (å·²è§£é”æ—¶é•¿é™åˆ¶) ---
async def generate_audio_file(text, filename="output.mp3"):
    # zh-CN-XiaoxiaoNeural æ˜¯ç›®å‰æœ€è‡ªç„¶çš„ä¸­æ–‡å¥³å£°
    communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
    await communicate.save(filename)

# --- 4. ä¾§è¾¹æ  (æ§åˆ¶å°) ---
with st.sidebar:
    st.title("âš¡ å·´å·´å¡”æ§åˆ¶å°")
    st.caption("V23 Unlocked Voice")
    
    # åŠŸèƒ½æ¨¡å¼é€‰æ‹©
    app_mode = st.selectbox("åˆ‡æ¢åŠŸèƒ½æ¨¡å¼", 
        ["ğŸ’¼ å•†ä¸šç­–åˆ’æ¡ˆ", "ğŸ“• å°çº¢ä¹¦çˆ†æ¬¾", "ğŸ“Š èŒåœºå‘¨æŠ¥å¤§å¸ˆ", "â¤ï¸ æƒ…æ„Ÿ/å“„äººä¸“å®¶"]
    )
    
    st.divider()
    
    # å•†ä¸šæ¨¡å¼ä¸“å±é€‰é¡¹
    if app_mode == "ğŸ’¼ å•†ä¸šç­–åˆ’æ¡ˆ":
        industry = st.selectbox("è¡Œä¸šèµ›é“", ["ğŸš€ AI/ç§‘æŠ€", "ğŸ›’ æ¶ˆè´¹/é›¶å”®", "ğŸ¥ åŒ»ç–—", "âš™ï¸ åˆ¶é€ "])
    
    style_mode = st.radio("AI è¯­æ°”é£æ ¼", ["ä¸“ä¸šç†æ€§", "æ¯’èˆŒå·´å·´å¡”", "æ¸©æŸ”è´´å¿ƒ", "çƒ­æƒ…æ¿€æ˜‚"])
    word_count = st.slider("ç”Ÿæˆå­—æ•°", 200, 3000, 800) # å­—æ•°ä¸Šé™è°ƒé«˜åˆ°3000
    enable_voice = st.toggle("ğŸ”Š å¼€å¯è¯­éŸ³æœ—è¯»", value=True)

# --- 5. æ™ºèƒ½ Prompt (æ ¸å¿ƒå¤§è„‘) ---
def get_prompt(mode):
    if mode == "ğŸ’¼ å•†ä¸šç­–åˆ’æ¡ˆ":
        return """ã€å¼ºåˆ¶ä¸­æ–‡ã€‘è¾“å‡ºå•†ä¸šç­–åˆ’æ¡ˆ(Markdown)ã€‚ç»“æ„ï¼šğŸ¯æ‘˜è¦ã€âš¡ç—›ç‚¹ã€ğŸ’æ–¹æ¡ˆã€ğŸ’°æ¨¡å¼ã€‚è¯·è¡¨ç°å¾—æå…·å•†ä¸šæ´å¯ŸåŠ›ã€‚"""
    elif mode == "ğŸ“• å°çº¢ä¹¦çˆ†æ¬¾":
        return """ä½ æ˜¯å°çº¢ä¹¦çˆ†æ¬¾åšä¸»ã€‚è¦æ±‚ï¼š1.æ ‡é¢˜å¸¦emojiæå…¶æŠ“çœ¼çƒã€‚2.æ­£æ–‡å¤šemojiï¼Œè¯­æ°”åƒé—ºèœœå®‰åˆ©ã€‚3.åŒ…å«ï¼šğŸŒŸäº®ç‚¹ã€ğŸ“æ„Ÿå—ã€ğŸ’¡é¿é›·ã€‚4.ç»“å°¾å¸¦#æ ‡ç­¾ã€‚"""
    elif mode == "ğŸ“Š èŒåœºå‘¨æŠ¥å¤§å¸ˆ":
        return """ä½ æ˜¯äº’è”ç½‘å¤§å‚P8ã€‚è¯·æŠŠç”¨æˆ·è¾“å…¥çš„ç®€å•å†…å®¹æ‰©å†™æˆé«˜å¤§ä¸Šçš„å‘¨æŠ¥ã€‚å¤šç”¨é»‘è¯ï¼šèµ‹èƒ½ã€é—­ç¯ã€æŠ“æ‰‹ã€æ²‰æ·€ã€å¤ç›˜ã€‚ç»“æ„ï¼šâœ…äº§å‡ºã€ğŸš§å¡ç‚¹ã€ğŸ“…è§„åˆ’ã€‚"""
    elif mode == "â¤ï¸ æƒ…æ„Ÿ/å“„äººä¸“å®¶":
        return """ä½ æ˜¯é¡¶çº§æƒ…æ„Ÿä¸“å®¶ã€‚å¦‚æœæ˜¯å“„äººï¼Œè¦æ¸©æŸ”ä½“è´´ï¼Œæä¾›æƒ…ç»ªä»·å€¼ï¼›å¦‚æœæ˜¯åˆ†ææ„Ÿæƒ…ï¼Œè¦ä¸€é’ˆè§è¡€ä½†å……æ»¡å…³æ€€ã€‚è¯·ç»™å‡ºå…·ä½“çš„è¡ŒåŠ¨å»ºè®®ã€‚"""

# --- 6. ä¸»ç•Œé¢ ---
st.title(f"{app_mode}") 

with st.form("universal_form"):
    if app_mode == "ğŸ’¼ å•†ä¸šç­–åˆ’æ¡ˆ":
        placeholder = "è¾“å…¥é¡¹ç›®ç‚¹å­ï¼Œå¦‚ï¼šç«æ˜Ÿå¥¶èŒ¶åº—..."
    elif app_mode == "â¤ï¸ æƒ…æ„Ÿ/å“„äººä¸“å®¶":
        placeholder = "è¾“å…¥æƒ…æ„Ÿå›°æƒ‘ï¼Œå¦‚ï¼šå¥³æœ‹å‹ç”Ÿæ°”äº†æ€ä¹ˆå“„ï¼Ÿ..."
    else:
        placeholder = "è¾“å…¥æ ¸å¿ƒä¸»é¢˜..."
        
    user_input = st.text_input("ğŸ’¡ è¯·è¾“å…¥å†…å®¹", placeholder=placeholder)
    submitted = st.form_submit_button("ğŸš€ ç«‹å³ç”Ÿæˆ")

# --- 7. æ‰§è¡Œé€»è¾‘ ---
if submitted and user_input:
    output_container = st.empty()
    full_text = ""
    
    # (1) AI ç”Ÿæˆ
    prompt_sys = get_prompt(app_mode)
    try:
        stream = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": f"{prompt_sys}\nè¯­æ°”:{style_mode}\nå­—æ•°:{word_count}"},
                {"role": "user", "content": user_input}
            ],
            stream=True
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_text += chunk.choices[0].delta.content
                output_container.markdown(full_text + "â–Œ")
        output_container.markdown(full_text)
        
        # (2) è¯­éŸ³æœ—è¯» (å…³é”®ä¿®æ”¹å¤„)
        if enable_voice:
            # æç¤ºè¯­å˜äº†ï¼Œå‘Šè¯‰ç”¨æˆ·å› ä¸ºå­—å¤šéœ€è¦ç­‰ä¸€ä¸‹
            with st.spinner("æ­£åœ¨åˆæˆå®Œæ•´è¯­éŸ³ (å­—æ•°è¾ƒå¤šï¼Œè¯·ç¨ç­‰ 5-10 ç§’)..."):
                
                # ğŸ”¥ ä¿®æ”¹å¤„ï¼šå»æ‰äº†åˆ‡ç‰‡é™åˆ¶ï¼Œç°åœ¨ä¼šè¯»å®Œå…¨æ–‡
                # ä¸ºäº†é˜²æ­¢ç‰¹æ®Šç¬¦å·å¯¼è‡´è¯­éŸ³åº“æŠ¥é”™ï¼Œè¿˜æ˜¯å»ºè®®ç®€å•æ¸…æ´—ä¸€ä¸‹
                read_text = full_text.replace("#", "").replace("*", "").replace("=", "").replace("-", "")
                
                # ç”Ÿæˆå®Œæ•´æ–‡ä»¶
                asyncio.run(generate_audio_file(read_text, "voice.mp3"))
                st.audio("voice.mp3", autoplay=True)
        
        # (3) å•†ä¸šå›¾è¡¨ (ä»…å•†ä¸šæ¨¡å¼æ˜¾ç¤º)
        if app_mode == "ğŸ’¼ å•†ä¸šç­–åˆ’æ¡ˆ":
            st.divider()
            st.subheader("ğŸ“Š å•†ä¸šæ•°æ®æ¨¡å‹")
            data = [100, 150, 230, 350, 500]
            df = pd.DataFrame(data, columns=["é¢„ä¼°è¥æ”¶(ä¸‡)"])
            st.area_chart(df)
